---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


```{r packages, include = F}

library(googleCloudStorageR)
library(dplyr)
library(tibble)

```

# bgg_data

loading historical data from BGG for predictive modeling and analysis

1. loads universe of game ids from [bgg activity club](http://bgg.activityclub.org/bggdata/thingids.txt])
2. submits batches of requests via [bggUtils](https://github.com/phenrickson/bggUtils)
3. stores responses on BigQuery and Google Cloud Storage

## targets

uses [targets](https://github.com/ropensci/targets) package to create pipeline

## data

batches of game ids

```{r, message = F, echo = F}

gcs_list_objects(versions = T) |>
        filter(name == 'raw/objects/game_ids') %>%
        arrange(desc(updated))

```
most recent batch of game ids submitted to API:

```{r most recent game ids, echo = F}

game_ids = targets::tar_read("game_ids")

batch_ts = attr(game_ids, "timestamp")

game_ids %>%
        add_column(batch_ts) %>%
        group_by(batch_ts, type) %>%
        summarize(
                n = n_distinct(id),
                .groups = 'drop'
        )
```
games retrieved from API:

```{r responses, echo = F}

gcs_list_objects(versions = T) |>
        filter(name == 'raw/objects/games') %>%
        arrange(desc(updated))

```

most recent batch of games:

```{r most recent games, message = F, echo = F}

games = targets::tar_read("games")

games %>% 
        group_by(batch_ts, type) %>% 
        summarize(n = n_distinct(game_id), .groups = 'drop')

```

