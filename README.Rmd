---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

# bgg_data

loading historical data from BGG for predictive modeling and analysis

1. loads universe of game ids from [bgg activity club](http://bgg.activityclub.org/bggdata/thingids.txt])
2. submits batches of requests via [bggUtils](https://github.com/phenrickson/bggUtils)
3. stores responses on BigQuery and Google Cloud Storage

## targets

uses [targets](https://github.com/ropensci/targets) package to create pipeline

```{r packages, include = F}

library(dplyr)
library(tibble)
library(qs)
library(googleCloudStorageR)
library(googleAuthR)

gcs_auth(
        json_file = Sys.getenv("GCS_AUTH_FILE")
)

googleCloudStorageR::gcs_global_bucket("bgg_data")

options(dplyr.print_max = 1e9)

```

## pipeline

```{r, message = F, results = "asis", echo = F}

cat(c("```mermaid", targets::tar_mermaid(), "```"), sep = "\n")

```


## data

last batch of game ids

```{r batches of ids, message = F, echo = F}

googleCloudStorageR::gcs_list_objects() |>
        filter(name == 'raw/objects/game_ids') %>%
        arrange(desc(updated))

```

most recent batch of game ids submitted to API:

```{r most recent game ids, echo = F}

game_ids = targets::tar_read("game_ids")

batch_ts = attr(game_ids, "timestamp")

game_ids %>%
        add_column(batch_ts) %>%
        group_by(batch_ts, type) %>%
        summarize(
                n = n_distinct(id),
                .groups = 'drop'
        )
```
most recent batch of games data loaded to GCS:

```{r most recent games, message = F, echo = F}

games = targets::tar_read("games")

games %>% 
        group_by(batch_ts, type) %>% 
        summarize(n = n_distinct(game_id), .groups = 'drop')

```

