---
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

# bgg_data

loading historical data from BGG for predictive modeling and analysis

1. loads universe of game ids from [bgg activity club](http://bgg.activityclub.org/bggdata/thingids.txt])
2. submits batches of requests via [bggUtils](https://github.com/phenrickson/bggUtils)
3. stores responses on BigQuery and Google Cloud Storage

## targets

uses [targets](https://github.com/ropensci/targets) package to create pipeline

```{r packages, include = F}

library(dplyr)
library(tibble)
library(qs)

# set scopes for authentication
googleCloudStorageR:::set_scopes()
# authenticate via encrypted json
googleCloudStorageR::gcs_auth(
        json_file = 
                gargle::secret_decrypt_json(
                        path = ".secrets/gcp-demos",
                        key = "GCS_AUTH_KEY"
                )
)

# set bucket
googleCloudStorageR::gcs_global_bucket(bucket = "bgg_data")

options(dplyr.print_max = 1e9)

```

## pipeline

```{r, message = F, results = "asis", echo = F}

cat(c("```mermaid", targets::tar_mermaid(), "```"), sep = "\n")

```


## data

batches of games requested from API

```{r message = F}

games_objs = 
        googleCloudStorageR::gcs_list_objects(versions = T, detail ='full') |>
        filter(name == 'raw/objects/games') |>
        select(name, bucket, generation, size, updated)

games_objs |>
        arrange(desc(updated))

```
